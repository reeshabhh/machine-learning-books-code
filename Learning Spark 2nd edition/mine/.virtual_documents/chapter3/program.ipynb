from pyspark import SparkContext as sc


from pyspark import SparkConf, SparkContext
conf = SparkConf().setMaster("local").setAppName("My App")
sc = SparkContext(conf = conf)


# In Python
# Create an RDD of tuples (name, age)
dataRDD = sc.parallelize([("Brooke", 20), ("Denny", 31), ("Jules", 30),
                            ("TD", 35), ("Brooke", 25)])
# Use map and reduceByKey transformations with their lambda
# expressions to aggregate and then compute average
agesRDD = (dataRDD
            .map(lambda x: (x[0], (x[1], 1)))
            .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1]))
            .map(lambda x: (x[0], x[1][0]/x[1][1])))
agesRDD.collect()


# In Python
from pyspark.sql import SparkSession
from pyspark.sql.functions import avg
# Create a DataFrame using SparkSession
spark = (SparkSession
    .builder
    .appName("AuthorsAges")
    .getOrCreate())
# sc = spark.sparkContext
# Create a DataFrame
data_df = spark.createDataFrame([("Brooke", 20), ("Denny", 31), ("Jules", 30),
     ("TD", 35), ("Brooke", 25)], ["name", "age"])
# Group the same names together, aggregate their ages, and compute an average
avg_df = data_df.groupBy("name").agg(avg("age"))
# Show the results of the final execution
avg_df.show()


spark = [SparkSession
    .builder
    .appName("AuthorsAges")
    .getOrCreate()]
spark


type(spark)


spark = (SparkSession
    .builder
    .appName("AuthorsAges")
    .getOrCreate())
spark


type(spark)


spark = SparkSession\
    .builder\
    .appName("AuthorsAges")\
    .getOrCreate()
spark


type(spark)


a = 2
b = 2,
c = (2)


type(a), type(b), type(c)


spark = (SparkSession
    .builder
    .appName("AuthorsAges")
    .getOrCreate(),)
spark


type(spark)


spark = (SparkSession
    .builder
    .appName("AuthorsAges")
    .getOrCreate())
spark


type(spark)


lst = 2,3,4,
5


lst


lst = (2,3,4,
5)


lst








# Programmatically defining a schema

# In Python
from pyspark.sql.types import *
schema = StructType([StructField("author", StringType(), False),
    StructField("title", StringType(), False),
    StructField("pages", IntegerType(), False)])
type(schema)


# defining schema using DDL

# In Python
from pyspark.sql import SparkSession
# Define schema for our data using DDL
schema = "Id INT, First STRING, Last STRING, Url STRING,\
    Published STRING, Hits INT, Campaigns ARRAY<STRING>"
type(schema)


# Create our static data
data = [
    [1, "Jules", "Damji", "https://tinyurl.1", "1/4/2016", 4535, ["twitter",
        "LinkedIn"]],
    [2, "Brooke","Wenig", "https://tinyurl.2", "5/5/2018", 8908, ["twitter",
        "LinkedIn"]],
    [3, "Denny", "Lee", "https://tinyurl.3", "6/7/2019", 7659, ["web",
        "twitter", "FB", "LinkedIn"]],
    [4, "Tathagata", "Das", "https://tinyurl.4", "5/12/2018", 10568,
        ["twitter", "FB"]],
    [5, "Matei","Zaharia", "https://tinyurl.5", "5/14/2014", 40578, ["web",
        "twitter", "FB", "LinkedIn"]],
    [6, "Reynold", "Xin", "https://tinyurl.6", "3/2/2015", 25568,
        ["twitter", "LinkedIn"]]
    ]


# Main program
if __name__ == "__main__":
# Create a SparkSession
    spark = (SparkSession
            .builder
            .appName("Example-3_6")
            .getOrCreate())
    # Create a DataFrame using the schema defined above
    blogs_df = spark.createDataFrame(data, schema)
    # Show the DataFrame; it should reflect our table above
    blogs_df.show()
    # Print the schema used by Spark to process the DataFrame
    print(blogs_df.printSchema())


# Create a DataFrame with the explicit schema specified.

from pyspark.sql.types import *
schema = StructType([
    StructField("name", StringType(), True),
    StructField("age", IntegerType(), True)])
spark.createDataFrame([('Alice', 1)], schema).show()


# Create a DataFrame with the schema in DDL formatted string.

spark.createDataFrame([('Alice', 1)], "name: string, age: int").show()


import pyspark.sql.types as T                   

# here is the traditional way to define a shema in PySpark
schema = T.StructType([
  T.StructField("col1", T.StringType(), True),
  T.StructField("col2", T.IntegerType(), True),
  T.StructField("col3", T.TimestampType(), True)
])
type(schema)


import pyspark.sql.types as T                   

# and here is the way using the helper function out of types
ddl_schema_string = "col1 string, col2 integer, col3 timestamp"
ddl_schema = T._parse_datatype_string(ddl_schema_string)
type(ddl_schema)


T._parse_datatype_string(ddl_schema_string)


#If you want to use this schema elsewhere in your code, simply execute# blogs_df.schema and it will return the schema definition:
blogs_df.schema


from pyspark.sql.functions import *

# Show columns and expressions
blogs_df.select(expr("Hits") * 2).show(2)
blogs_df.select(col("Hits") * 2).show(2)
blogs_df.select(expr("Hits * 2")).show(2)
# show heavy hitters
blogs_df.withColumn("Big Hitters", (expr("Hits > 10000"))).show()


blogsDF = blogs_df


# // Use an expression to compute big hitters for blogs
# // This adds a new column, Big Hitters, based on the conditional expression
blogsDF.withColumn("Big Hitters", (expr("Hits > 10000"))).show()


# // Concatenate three columns, create a new column, and show the
# // newly created concatenated column
(blogsDF
.withColumn("AuthorsId", (concat(expr("First"), expr("Last"), expr("Id"))))
.select(col("AuthorsId"))
.show(4))


# // These statements return the same value, showing that
# // expr is the same as a col method call
blogsDF.select(expr("Hits")).show(2)
blogsDF.select(col("Hits")).show(2)
blogsDF.select("Hits").show(2)


# // Sort by column "Id" in descending order
blogsDF.sort(col("Id").desc()).show()
blogsDF.sort(expr("Id").desc()).show()





# In Python
from pyspark.sql import Row
blog_row = Row(6, "Reynold", "Xin", "https://tinyurl.6", 255568, "3/2/2015",
    ["twitter", "LinkedIn"])
# access using index for individual items
blog_row[1], blog_row


# In Python
rows = [Row("Matei Zaharia", "CA"), Row("Reynold Xin", "CA")]
authors_df = spark.createDataFrame(rows, ["Authors", "State"])
authors_df.show()


rows


rows[0]


rows[0][0]


# In Python
df = spark.range(0, 10000, 1, 8) # this code will create a DataFrame of 10,000 integers distributed over eight parti‚Äê
# tions in memory 
print(df.rdd.getNumPartitions()) # it should print 8





# In Python, define a schema
from pyspark.sql.types import *
# Programmatic way to define a schema
fire_schema = StructType(
    [StructField('CallNumber', IntegerType(), True),
    StructField('UnitID', StringType(), True),
    StructField('IncidentNumber', IntegerType(), True),
    StructField('CallType', StringType(), True),
    StructField('CallDate', StringType(), True),
    StructField('WatchDate', StringType(), True),
    StructField('CallFinalDisposition', StringType(), True),
    StructField('AvailableDtTm', StringType(), True),
    StructField('Address', StringType(), True),
    StructField('City', StringType(), True),
    StructField('Zipcode', IntegerType(), True),
    StructField('Battalion', StringType(), True),
    StructField('StationArea', StringType(), True),
    StructField('Box', StringType(), True),
    StructField('OriginalPriority', StringType(), True),
    StructField('Priority', StringType(), True),
    StructField('FinalPriority', IntegerType(), True),
    StructField('ALSUnit', BooleanType(), True),
    StructField('CallTypeGroup', StringType(), True),
    StructField('NumAlarms', IntegerType(), True),
    StructField('UnitType', StringType(), True),
    StructField('UnitSequenceInCallDispatch', IntegerType(), True),
    StructField('FirePreventionDistrict', StringType(), True),
    StructField('SupervisorDistrict', StringType(), True),
    StructField('Neighborhood', StringType(), True),
    StructField('Location', StringType(), True),
    StructField('RowID', StringType(), True),
    StructField('Delay', FloatType(), True)]
)

# Use the DataFrameReader interface to read a CSV file
sf_fire_file = "sf-fire-calls.csv"
fire_df = spark.read.csv(sf_fire_file, header=True, schema=fire_schema)


print(fire_df.take(1))


fire_df.printSchema()


print(fire_df.show(5, truncate=False))


from IPython.core.display import HTML
display(HTML("<style>pre { white-space: pre !important; }</style>"))


fire_df.take(1)


fire_df.show(2)


fire_df.show(2, truncate=False)


%%html
<style>
div.output_area pre {
    white-space: pre;
}


fire_df.show(2, truncate=True)


fire_df.show(2, truncate=False)


!python --version


!java --version


# In Python to save as a Parquet file
parquet_path = "written_data/parquet_file_1"
fire_df.write.mode("overwrite").format("parquet").save(parquet_path) # parquet file will be written inside written_data/parquet_file_1 folder

fire_df.write.mode("overwrite").format("parquet").save("written_data/parquet_file_2") # parquet file will be written inside written_data/parquet_file_2 folder

# Another way
fire_df.write.mode("overwrite").parquet("written_data/parquet_file_3") # parquet file will be written inside written_data/parquet_file_3 folder


# In Python to save as a csv file
csv_path = "written_data/csv_file_1"
fire_df.write.mode("overwrite").format("csv").save(csv_path) # csv file will be written inside written_data/csv_file_1 folder

fire_df.write.mode("overwrite").format("csv").save("written_data/csv_file_2") # csv file will be written inside written_data/csv_file_2 folder

# Another way
fire_df.write.mode("overwrite").csv("written_data/csv_file_3") # csv file will be written inside written_data/csv_file_3 folder


# In Python Writing to SQL table named parquet_table
# 
# parquet_table = parquet_table # name of the table
# fire_df.write.format("parquet").saveAsTable(parquet_table)





# In Python
few_fire_df = (fire_df
.select("IncidentNumber", "AvailableDtTm", "CallType")
.where(col("CallType") != "Medical Incident"))
few_fire_df.show(5, truncate=False)


# In Python, return number of distinct types of calls using countDistinct()
from pyspark.sql.functions import *
(fire_df.select("CallType")
.where(col("CallType").isNotNull())
.agg(countDistinct("CallType").alias("DistinctCallTypes"))
.show())


# In Python, filter for only distinct non-null CallTypes from all the rows
(fire_df
.select("CallType")
.where(col("CallType").isNotNull())
.distinct()
.show(10, False))





# In Python
new_fire_df = fire_df.withColumnRenamed("Delay", "ResponseDelayedinMins")
(new_fire_df
.select("ResponseDelayedinMins")
.where(col("ResponseDelayedinMins") > 5)
.show(5, False))





# In Python
fire_ts_df = (new_fire_df
    .withColumn("IncidentDate", to_timestamp(col("CallDate"), "MM/dd/yyyy"))
    .drop("CallDate")
    .withColumn("OnWatchDate", to_timestamp(col("WatchDate"), "MM/dd/yyyy"))
    .drop("WatchDate")
    .withColumn("AvailableDtTS", to_timestamp(col("AvailableDtTm"), "MM/dd/yyyy hh:mm:ss a"))
    .drop("AvailableDtTm"))


# Select the converted columns
(fire_ts_df
    .select("IncidentDate", "OnWatchDate", "AvailableDtTS")
    .show(5, False))


(fire_ts_df
    .select(year('IncidentDate'))
    .distinct()
    .orderBy(year('IncidentDate'))
    .show())


(fire_ts_df.printSchema())


(fire_ts_df
.select(col("CallType"), col("CallType"))# , count(col("CallType")))
.where(col("CallType").isNotNull())
.groupBy("CallType").
.count()
# .orderBy("count", ascending=False)
.show(n=10, truncate=False))


(fire_ts_df
.createOrReplaceTempView("fire_ts_df_tbl")
)


print(fire_ts_df.columns)


(spark.sql
("SELECT * FROM FIRE_TS_DF_TBL limit 10")
).show()


(spark.sql('SELECT YEAR(IncidentDate) AS YEAR, count(IncidentNumber) AS CNT FROM FIRE_TS_DF_TBL GROUP BY YEAR(IncidentDate) ORDER BY CNT DESC limit 5')).show()


(spark.sql('SELECT TRIM(TO_DATE(IncidentDate, "YYYY")) YEAR, count(IncidentNumber) CNT FROM FIRE_TS_DF_TBL GROUP BY TRIM(TO_DATE(IncidentDate, "YYYY")) ORDER BY CNT LIMIT 5')).show()


(spark.sql
("SELECT distinct(IncidentNumber) FROM FIRE_TS_DF_TBL limit 10")
).show()


(spark.sql
("SELECT distinct(CALLNUMBER) FROM FIRE_TS_DF_TBL limit 10")
).show()


# df.groupBy(‚Äúcol1‚Äù).agg({‚Äúcol2‚Äù: ‚Äúavg‚Äù}).orderBy(‚Äúcol1‚Äù)


(fire_ts_df
.select("CallType")
.where(col("CallType").isNotNull())
.groupBy("CallType")
.count()
.orderBy("count", ascending=False)
.show(n=5, truncate=False))


(fire_ts_df
    # .select("CallType")
    .where(col("CallType").isNotNull())
    .groupBy("CallType")
    .count()
    .orderBy("count", ascending=False)
.show(n=5, truncate=False))


(fire_ts_df
    .groupBy(year("IncidentDate"))
    .count()
).show(4)


(fire_ts_df
    .groupBy(year("IncidentDate").alias("Year"), "CallType")
    .count().alias("Count of incidents")
     .orderBy("year", ascending = False)
).show(4)


(fire_ts_df
 .select("IncidentDate", "CallType")
 .groupBy(year("IncidentDate").alias("Year"), "incidentdate")
    .count()
    .orderBy("count", ascending=False)
).show(3)


fire_ts_df.printSchema()


# df.groupBy(‚Äúcol1‚Äù).agg({‚Äúcol2‚Äù: ‚Äúavg‚Äù}).orderBy(‚Äúcol1‚Äù)


(
    fire_ts_df
    .select(avg("NumAlarms"))
    # .avg()
).show(3)


(
    fire_ts_df
    .select(count("NumAlarms"))
    # .avg()
).show(3)


(
    fire_ts_df
    .where(col("NumAlarms").isNotNull())
    .groupBy("NumAlarms")
    .count()
).show(3)


(
    fire_ts_df
    .select(count("*"))
    # .avg()
).show(3)


(
    fire_ts_df
    .groupBy(day("IncidentDate"))
    .count()
).show(3)


(fire_ts_df
.groupBy(year("IncidentDate"))
# .mean()
 .count()
.orderBy("count", ascending=False)
).show(3)


(fire_ts_df
.groupBy(day("IncidentDate"))
# .mean()
 .count()
.orderBy("count", ascending=False)
).show(3)


# group by date group by call type - see average callnumber on that date of call type


# see year wise average number of calls


# AVG
df = spark.sql("""SELECT ACCT,AMT,TXN_DT FROM VALUES 
(101,10.01, DATE'2021-01-01'),
(101,102.01, DATE'2021-01-01'),
(102,93., DATE'2021-01-01'),
(103,913.1, DATE'2021-01-02'),
(102,913.1, DATE'2021-01-02'),
(101,900.56, DATE'2021-01-03')
AS TXN(ACCT,AMT,TXN_DT) """)


df.show(1)


print(fire_ts_df.columns)


df.groupBy("TXN_DT").agg(avg("AMT").alias("AVG_AMT")).show(1)


(
    fire_ts_df
    .groupBy(day("IncidentDate"))
    .agg(sum("NumAlarms"))
).show(3)


(
    fire_ts_df
    .groupBy(day("IncidentDate"))
    .agg(avg("NumAlarms"))
).show(3)


(
    fire_ts_df
    .groupBy(day("IncidentDate"))
    .agg(count(day("IncidentDate")))
).show(3)


3400/3360


(
    (
    fire_ts_df
    .groupBy(day("IncidentDate"))
    .mean("NumAlarms")
    )
.show(3)
)


(
    (
    fire_ts_df
    .groupBy(day("IncidentDate"))
    .avg("NumAlarms")
    )
.show(3)
)


(
    (
    fire_ts_df
    .groupBy(day("IncidentDate"))
    .sum("NumAlarms")
    )
.show(3)
)






